{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling with RNNs\n",
    "For this notebook, you do not need to write any code. Instead, follow along with the notebook, familiarizing yourself with using Keras to generate RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Just as in the other notebooks, we will begin with importing the needed modules and reading in the training data. We'll also be borrowing some code from the other notebook to remove infrequent words and cut down our vocabulary size.\n",
    "\n",
    "If you do not have Keras or Tensorflow you can install them using the terminal following these instructions.\n",
    "\n",
    "[Install Keras](https://keras.io/#installation)\n",
    "\n",
    "[Install Tensorflow](https://www.tensorflow.org/install/install_mac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers import Dense, SimpleRNN, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Reading in the training data; we'll be taking a smaller set to reduce training time\n",
    "with open(\"headlines.train\", 'r') as f:\n",
    "    headlines_train = f.readlines()[:100000]\n",
    "\n",
    "# Removing excess punctuation and newline\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "headlines_train = [regex.sub('', h.split(\"\\n\")[0]) for h in headlines_train]\n",
    "\n",
    "# Define the unk, start and stop tokens\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "START_TOKEN = \"<START>\"\n",
    "STOP_TOKEN = \"<STOP>\"\n",
    "\n",
    "# We'll be borrowing some code from the other notebook to trim down the vocabulary a bit\n",
    "def count_unigrams(text, unigram_dict):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    :param unigram_dict: A dictionary containing unigrams as keys and their respective counts as values\n",
    "    \"\"\"\n",
    "    tokens = [START_TOKEN] + text.split(\" \") + [STOP_TOKEN]\n",
    "    for i in range(len(tokens)):\n",
    "        unigram = tokens[i]\n",
    "        if unigram not in unigram_dict:\n",
    "            unigram_dict[unigram] = 1\n",
    "        else:\n",
    "            unigram_dict[unigram] += 1\n",
    "\n",
    "min_freq = 3 # The minimum word frequency to be present in the vocabulary\n",
    "\n",
    "# The following are used to keep track of and remove infrequent words\n",
    "low_freq = set()\n",
    "all_words = {}\n",
    "\n",
    "def replace_text_train(text):\n",
    "    return \" \".join([UNK_TOKEN if t in low_freq else t for t in text.split()])\n",
    "\n",
    "# Finding all words with low frequency\n",
    "for h in headlines_train:\n",
    "    count_unigrams(h, all_words)\n",
    "for word, count in all_words.items():\n",
    "    if count <= min_freq:\n",
    "        low_freq.add(word)\n",
    "# Replacing low frequency words from training dataset with UNK\n",
    "headlines_train_clean = [replace_text_train(h) for h in headlines_train]\n",
    "\n",
    "# Build vocabulary and make a mapping from index to word for generation\n",
    "vocab = set([item for sublist in map(lambda x: x.split(\" \"), headlines_train_clean) for item in sublist])\n",
    "vocab.add(STOP_TOKEN)\n",
    "vocab_list = list(vocab)\n",
    "word_to_index = {vocab_list[i]: i for i in range(len(vocab_list))}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our RNN, we be first converting our text into GloVe word embeddings before giving it as input. We'll also need to define some parameters which will be used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading in GloVe embeddings as save them as a dictionary\n",
    "with open(\"glove_embeddings.txt\", 'r') as f:\n",
    "    gloves = [t.split(\" \") for t in f.readlines()]\n",
    "    gloves_dict = {t[0]: np.array(t[1:]) for t in gloves}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters to used for the batch generator and model\n",
    "vocab_size = len(index_to_word.keys())\n",
    "sent_len = max([len(h.split(\" \")) for h in headlines_train_clean]) + 1\n",
    "glove_dim = next(iter(gloves_dict.values())).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Batches\n",
    "First we'll need to turn the headlines into data samples (where each sample is an output word given the entire history of previous words in the headline). To do this, we will iterate through all the headlines and through each word within the headline to get (history, word) pairs as our inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for h in headlines_train_clean:\n",
    "    # Pad the text in the beginning with start tokens\n",
    "    text = [START_TOKEN for _ in range(sent_len)] + h.split(\" \") + [STOP_TOKEN]\n",
    "    for i in range(len(text) - sent_len):\n",
    "        data.append((text[i:i+sent_len], text[i+sent_len]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras allows batches of data to be fed into the RNN through a generator, so we'll make such a generator to process the data and package it nicely for the model to use during the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the data generator and model\n",
    "batch_size = 512\n",
    "num_batches = -(-len(data) // batch_size)\n",
    "\n",
    "def sample_generator():\n",
    "    while True:\n",
    "        random.shuffle(data)\n",
    "        for i in range(num_batches):\n",
    "            batch_input = np.zeros((batch_size, sent_len, glove_dim))\n",
    "            batch_label = np.zeros((batch_size, vocab_size))\n",
    "            for j in range(batch_size):\n",
    "                idx = j + i*batch_size\n",
    "                history, word = data[j]\n",
    "                for k in range(len(history)):\n",
    "                    if history[k] in gloves_dict:\n",
    "                        batch_input[j,k,:] = gloves_dict[history[k]]\n",
    "                batch_label[j,word_to_index[word]] = 1\n",
    "            yield batch_input, batch_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "Keras is a high-level machine learning library that greatly simplifies building and training neural networks. It does the forward and backward pass, as well as other implementation details, all for you; you just need to declare what kind of layers you would like to add. To read the API and view some tutorials on how to use Keras, visit https://keras.io/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 128\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_neurons, input_shape=(sent_len, glove_dim)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code defines a few functions to generate sentences from the RNN. The `sample` and `generate_headline` functions behave in a similar way to the `sample_word` function that you had implemented and the `generate_headline` function implemented for you in the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_headline():\n",
    "    end_sentence = False\n",
    "    sent = np.zeros((sent_len, glove_dim))\n",
    "    \n",
    "    generated = []\n",
    "    curr_len = 0\n",
    "    while not end_sentence:\n",
    "        sent_input = np.expand_dims(sent[-sent_len:sent.shape[0]], axis=0)\n",
    "        word_probs = model.predict(sent_input, verbose=0)\n",
    "        next_word = sample(np.squeeze(word_probs, axis=0))\n",
    "        if next_word == word_to_index[STOP_TOKEN] or curr_len == sent_len:\n",
    "            end_sentence = True\n",
    "            print(' '.join(generated))\n",
    "        else:\n",
    "            if index_to_word[next_word] in gloves_dict:\n",
    "                word_embeded = gloves_dict[index_to_word[next_word]]\n",
    "            else:\n",
    "                word_embeded = np.zeros(glove_dim)\n",
    "            sent = np.concatenate((sent, np.expand_dims(word_embeded, axis=0)), axis=0)\n",
    "            generated.append(index_to_word[next_word])\n",
    "            curr_len += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin training, let's first look at what kind of headlines an untrained RNN generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    generate_headline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Training\n",
    "Now that we've constructed our RNN, we can begin training. Be aware that this may take upwards of half an hour to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    for i in range(3):\n",
    "        generate_headline()\n",
    "        print()\n",
    "\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.fit_generator(sample_generator(), num_batches, 3,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've finally finished training our RNN! Let's see what kind of headlines we can generate now. \n",
    "\n",
    "List the three headlines generated after running the cell below in the RNN Language Model section of the `bigram_language_model.ipynb` notebook. You do not need to submit this notebook; only submit `bigram_language_model.ipynb` and the its executed PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    generate_headline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
